{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Sample', ' original_spk', ' gender', ' original_time', ' type_voc', ' start_voc', 'end_voc'])\n",
    "file1 = open(\"vocalizationcorpus/labels.txt\",\"r\")\n",
    "for aline in file1:\n",
    "    values = aline.split(',')\n",
    "    values[-1] = values[-1].strip(\"\\n\")\n",
    "    df.loc[len(df)] = values[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():    \n",
    "    t1 = float(row[1][5]) * 1000\n",
    "    t2 = float(row[1][6]) * 1000\n",
    "    duration = t2 - t1\n",
    "    newAudio = AudioSegment.from_wav(\"vocalizationcorpus/data/\" + row[1][0] + \".wav\")\n",
    "    newAudio = newAudio[t1:t2]\n",
    "    if row[1][4] == 'filler' and duration > 100:\n",
    "        newAudio.export(\"Filler Words/\" + row[1][0] + \".wav\", format=\"wav\")\n",
    "    if row[1][4] == 'laughter' and duration > 100:\n",
    "        newAudio.export(\"Laughter/\" + row[1][0] + \".wav\", format=\"wav\")\n",
    "    print(row[1][0], row[1][4], duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCall\n",
    "\n",
    "Step 1: Feature Extraction - after collection audio data, extract the features  \n",
    "Step 2: Speaker Clustering - identify who is speaker 1 and who is speaker 2  \n",
    "Step 3: Training - train your model to classify the data into: Speech, Laughter, Filler Words  \n",
    "Step 4: User Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from pyAudioAnalysis import audioSegmentation\n",
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Classifies audio into 2 speakers + plot functionality\n",
    "def speakerDiarization(filename, plot = False):\n",
    "    speakers = audioSegmentation.speakerDiarization(filename, 2, plot_res=False)                                            \n",
    "    dataframe=pd.DataFrame(speakers, columns=['category'])\n",
    "    dataframe[\"seconds\"] = np.linspace(0,dataframe.shape[0]/5,dataframe.shape[0])\n",
    "    \n",
    "    if plot == True:\n",
    "        generatePlot(dataframe)\n",
    "\n",
    "# Plot functionality for speakerDiarization\n",
    "def generatePlot(dataframe):\n",
    "    figure(num=None, figsize=(15, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.style.use('ggplot')\n",
    "    x = dataframe[\"seconds\"]\n",
    "    plt.xticks(np.arange(min(x), max(x)+1, 5.0))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks([0, 1])\n",
    "    plt.xlabel(\"Seconds\")\n",
    "    plt.ylabel(\"Speakers\")\n",
    "    plt.title(\"Speaker Diarization\")\n",
    "    plt.plot(dataframe[\"seconds\"], dataframe[\"category\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fs, x] = audioBasicIO.readAudioFile(\"Sinclair.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = audioSegmentation.speakerDiarization(\"Sinclair.wav\", 2, plot_res=False)                                            \n",
    "dataframe=pd.DataFrame(speakers, columns=['category'])\n",
    "dataframe[\"seconds\"] = np.linspace(0,dataframe.shape[0]/5,dataframe.shape[0])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Diarization \n",
    "\n",
    "This algorithm allows us to know who is speaking in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerDiarization(\"Sinclair.wav\", plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification!\n",
    "\n",
    "We extract mid-term features. We use a long-term averaging of the mid-term features, leading to 1 feature vector for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.featureAndTrain([\"Filler Words\",\"Laughter\", \"Speech\"], 0.1, 0.1, aT.shortTermWindow, aT.shortTermStep, \"randomforest\", \"rf\", perTrain=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.featureAndTrain([\"Filler Words\",\"Laughter\", \"Speech\"], 0.1, 0.1, aT.shortTermWindow, aT.shortTermStep, \"gradientboosting\", \"gb\",perTrain=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.featureAndTrain([\"Filler Words\",\"Laughter\", \"Speech\"], 0.1, 0.1, aT.shortTermWindow, aT.shortTermStep, \"svm_rbf\", \"svm\",perTrain=0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open(\"svm\",'rb')\n",
    "svm = pickle.load(file)\n",
    "print(svm.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.fileClassification(\"Test/laugh.wav\", \"svm\", \"svm_rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "[fs, x] = audioBasicIO.readAudioFile(\"Laughter/S0005.wav\")\n",
    "x = audioBasicIO.stereo2mono(x)\n",
    "\n",
    "write('Test/laugh.wav', fs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneAnalyzer(filename):\n",
    "    [fs, x] = audioBasicIO.readAudioFile(filename)\n",
    "    x = audioBasicIO.stereo2mono(x)\n",
    "\n",
    "    speakers = audioSegmentation.speakerDiarization(filename, 2, plot_res=False)                                            \n",
    "    dataframe=pd.DataFrame(speakers, columns=['category'])\n",
    "    dataframe[\"seconds\"] = np.linspace(0,dataframe.shape[0]/5,dataframe.shape[0])\n",
    "\n",
    "    windowSize = 0.4\n",
    "    startWin = 0\n",
    "    endWin = round(fs * windowSize)\n",
    "    shift = round(fs * windowSize)\n",
    "    timer = 0\n",
    "\n",
    "    while endWin < len(x):\n",
    "        windowSlice = x[startWin:endWin]\n",
    "        mt_win = 0.1\n",
    "        mt_step = 0.1\n",
    "        st_win = aT.shortTermWindow # 0.05\n",
    "        st_step = aT.shortTermStep # 0.05\n",
    "        [classifier, MEAN, STD, classNames, mt_win, mt_step, st_win, st_step, compute_beat] = aT.load_model(\"svm\")\n",
    "        [mt_term_feats, st_features, _] = audioFeatureExtraction.mtFeatureExtraction(windowSlice, fs, round(mt_win * fs), round(mt_step * fs), round(fs * st_win), round(fs * st_step))\n",
    "        mt_term_feats = mt_term_feats.mean(axis=1) \n",
    "        curFV = (mt_term_feats - MEAN) / STD\n",
    "        R = classifier.predict(curFV.reshape(1,-1))[0]\n",
    "        P = classifier.predict_proba(curFV.reshape(1,-1))[0]\n",
    "        startWin += shift\n",
    "        endWin += shift\n",
    "        timer += windowSize\n",
    "        speakerID = int(dataframe[dataframe[\"seconds\"] > timer].iloc[0,0])\n",
    "        print(\"Speaker: \", speakerID, \"Classification: \", R, \"Probability: \", round(max(P), 2))\n",
    "\n",
    "phoneAnalyzer(\"Sinclair.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
