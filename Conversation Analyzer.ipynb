{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "df = pd.DataFrame(columns=['Sample', ' original_spk', ' gender', ' original_time', ' type_voc', ' start_voc', 'end_voc'])\n",
    "file1 = open(\"vocalizationcorpus/labels.txt\",\"r\")\n",
    "for aline in file1:\n",
    "    values = aline.split(',')\n",
    "    values[-1] = values[-1].strip(\"\\n\")\n",
    "    df.loc[len(df)] = values[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.iterrows():    \n",
    "    t1 = float(row[1][5]) * 1000\n",
    "    t2 = float(row[1][6]) * 1000\n",
    "    duration = t2 - t1\n",
    "    newAudio = AudioSegment.from_wav(\"vocalizationcorpus/data/\" + row[1][0] + \".wav\")\n",
    "    newAudio = newAudio[t1:t2]\n",
    "    if row[1][4] == 'filler' and duration > 100:\n",
    "        newAudio.export(\"Filler Words/\" + row[1][0] + \".wav\", format=\"wav\")\n",
    "    if row[1][4] == 'laughter' and duration > 100:\n",
    "        newAudio.export(\"Laughter/\" + row[1][0] + \".wav\", format=\"wav\")\n",
    "    print(row[1][0], row[1][4], duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionAudio(filename, name):\n",
    "    newAudio = AudioSegment.from_wav(filename)\n",
    "    t1 = 0\n",
    "    t2 = 1000 * 1\n",
    "    while t2 < len(newAudio):\n",
    "        print(t2/1000)\n",
    "        newAudio[t1:t2].export('Laughter/' + name + '-laugh-' + str(t2) + '.wav', format=\"wav\")\n",
    "        t1 += 1000 * 1\n",
    "        t2 += 1000 * 1\n",
    "#partitionAudio(\"Silence.wav\")\n",
    "\n",
    "import os\n",
    "\n",
    "directory = os.fsencode(\"Laughter\")\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    print(filename)\n",
    "    partitionAudio(\"Laughter/\" + filename, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCall\n",
    "\n",
    "Step 1: Feature Extraction - after collection audio data, extract the features  \n",
    "Step 2: Speaker Clustering - identify who is speaker 1 and who is speaker 2  \n",
    "Step 3: Training - train your model to classify the data into: Speech, Laughter, Filler Words  \n",
    "Step 4: User Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from pyAudioAnalysis import audioSegmentation\n",
    "from pyAudioAnalysis import audioTrainTest as aT\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Classifies audio into 2 speakers + plot functionality\n",
    "def speakerDiarization(filename, plot = False):\n",
    "    speakers = audioSegmentation.speakerDiarization(filename, 2, plot_res=False)                                            \n",
    "    dataframe=pd.DataFrame(speakers, columns=['category'])\n",
    "    dataframe[\"seconds\"] = np.linspace(0,dataframe.shape[0]/5,dataframe.shape[0])\n",
    "    \n",
    "    if plot == True:\n",
    "        generatePlot(dataframe)\n",
    "    return dataframe\n",
    "\n",
    "# Plot functionality for speakerDiarization\n",
    "def generatePlot(dataframe):\n",
    "    figure(num=None, figsize=(15, 3), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.style.use('ggplot')\n",
    "    x = dataframe[\"seconds\"]\n",
    "    plt.xticks(np.arange(min(x), max(x)+1, 5.0))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks([0, 1])\n",
    "    plt.xlabel(\"Seconds\")\n",
    "    plt.ylabel(\"Speakers\")\n",
    "    plt.title(\"Speaker Diarization\")\n",
    "    plt.plot(dataframe[\"seconds\"], dataframe[\"category\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fs, x] = audioBasicIO.readAudioFile(\"Sinclair.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = audioSegmentation.speakerDiarization(\"Sinclair.wav\", 2, plot_res=False)                                            \n",
    "dataframe=pd.DataFrame(speakers, columns=['category'])\n",
    "dataframe[\"seconds\"] = np.linspace(0,dataframe.shape[0]/5,dataframe.shape[0])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Diarization \n",
    "\n",
    "This algorithm allows us to know who is speaking in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerDiarization(\"Sinclair.wav\", plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification!\n",
    "\n",
    "We extract mid-term features. We use a long-term averaging of the mid-term features, leading to 1 feature vector for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.featureAndTrain([\"Filler Words\",\"Laughter\", \"Speech\"], 0.1, 0.1, aT.shortTermWindow, aT.shortTermStep, \"randomforest\", \"rf\", perTrain=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.featureAndTrain([\"Filler Words\",\"Laughter\", \"Speech\"], 0.1, 0.1, aT.shortTermWindow, aT.shortTermStep, \"gradientboosting\", \"gb\",perTrain=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aT.featureAndTrain([\"Filler Words\",\"Laughter\", \"Speech\", \"Background\"], 0.2, 0.2, aT.shortTermWindow, aT.shortTermStep, \"svm_rbf\", \"svm\",perTrain=0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open(\"svm\",'rb')\n",
    "svm = pickle.load(file)\n",
    "print(svm.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Speaker Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = speakerDiarization(\"Sinclair.wav\", plot = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"ground_Truth\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to annotate who is speaking by the second.\n",
    "df1 = dataframe[dataframe[\"seconds\"].between(0, 7)][\"ground_Truth\"].replace(0, 1) # Speaker 1 is Joe Rogan\n",
    "df2 = dataframe[dataframe[\"seconds\"].between(7, 37)][\"ground_Truth\"] # Speaker 0 by default \n",
    "df3 = dataframe[dataframe[\"seconds\"].between(37, 38)][\"ground_Truth\"].replace(0, 1)\n",
    "df4 = dataframe[dataframe[\"seconds\"].between(38, 67)][\"ground_Truth\"]\n",
    "df5 = dataframe[dataframe[\"seconds\"].between(67, 83)][\"ground_Truth\"].replace(0, 1)\n",
    "df6 = dataframe[dataframe[\"seconds\"].between(83, 88)][\"ground_Truth\"]\n",
    "df7 = dataframe[dataframe[\"seconds\"].between(88, 106)][\"ground_Truth\"].replace(0, 1)\n",
    "df8 = dataframe[dataframe[\"seconds\"].between(106, 113)][\"ground_Truth\"]\n",
    "df9 = dataframe[dataframe[\"seconds\"].between(113, 117)][\"ground_Truth\"].replace(0, 1)\n",
    "df10 = dataframe[dataframe[\"seconds\"].between(117, 118)][\"ground_Truth\"]\n",
    "df11 = dataframe[dataframe[\"seconds\"].between(118, 135)][\"ground_Truth\"].replace(0, 1)\n",
    "df12 = dataframe[dataframe[\"seconds\"].between(135, 185)][\"ground_Truth\"]\n",
    "df13 = dataframe[dataframe[\"seconds\"].between(185, 194)][\"ground_Truth\"].replace(0, 1)\n",
    "df14 = dataframe[dataframe[\"seconds\"].between(194, 209)][\"ground_Truth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine everything!\n",
    "labels = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your annotations as a column\n",
    "dataframe[\"ground_Truth\"] = labels\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn provides a quick way to check accuracy between ground truth and category.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(list(dataframe[\"ground_Truth\"]), list(dataframe[\"category\"].astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phoneAnalyzer(filename, df):\n",
    "    [fs, x] = audioBasicIO.readAudioFile(filename)\n",
    "    x = audioBasicIO.stereo2mono(x)\n",
    "                             \n",
    "    dataframe=pd.DataFrame(speakers, columns=['category'])\n",
    "    dataframe[\"seconds\"] = np.linspace(0,dataframe.shape[0]/5,dataframe.shape[0])\n",
    "\n",
    "    windowSize = 1\n",
    "    startWin = 0\n",
    "    endWin = round(fs * windowSize)\n",
    "    shift = round(fs * windowSize)\n",
    "    timer = 0\n",
    "\n",
    "    while endWin < len(x):\n",
    "        windowSlice = x[startWin:endWin]\n",
    "        mt_win = 0.2\n",
    "        mt_step = 0.2\n",
    "        st_win = aT.shortTermWindow # 0.05\n",
    "        st_step = aT.shortTermStep # 0.05\n",
    "        [classifier, MEAN, STD, classNames, mt_win, mt_step, st_win, st_step, compute_beat] = aT.load_model(\"svm\")\n",
    "        [mt_term_feats, st_features, _] = audioFeatureExtraction.mtFeatureExtraction(windowSlice, fs, round(mt_win * fs), round(mt_step * fs), round(fs * st_win), round(fs * st_step))\n",
    "        mt_term_feats = mt_term_feats.mean(axis=1) \n",
    "        curFV = (mt_term_feats - MEAN) / STD\n",
    "        R = classifier.predict(curFV.reshape(1,-1))[0]\n",
    "        P = classifier.predict_proba(curFV.reshape(1,-1))[0]\n",
    "        startWin += shift\n",
    "        endWin += shift\n",
    "        timer += windowSize\n",
    "        speakerID = int(df[df[\"seconds\"] > timer].iloc[0,0])\n",
    "        print(\"Time (Seconds): \", timer, \"Speaker: \", speakerID, \"Classification: \", R, \"Probability: \", round(max(P), 2))\n",
    "\n",
    "phoneAnalyzer(\"toastmaster.wav\", dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import keyboard\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"file.wav\"\n",
    " \n",
    "audio = pyaudio.PyAudio()\n",
    " \n",
    "# start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                rate=RATE, input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "print(\"Recording...\")\n",
    "frames = []\n",
    "    \n",
    "while True:  # making a loop\n",
    "    try:  # used try so that if user pressed other than the given key error will not be shown\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            print('You Pressed A Key!')\n",
    "            break  # finishing the loop\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        break \n",
    "        \n",
    "print(\"Finished Recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    " \n",
    "waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(frames))\n",
    "waveFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
